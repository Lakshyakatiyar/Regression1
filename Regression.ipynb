{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea2a992-e34e-4911-b09b-48fe7f415b86",
   "metadata": {},
   "source": [
    "1.Simple Linear Regression:\n",
    "Simple linear regression models the relationship between two variables by fitting a linear equation to observed data. One variable is independent (predictor), and the other is dependent (response). The equation of a simple linear regression line is:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ϵ\n",
    "\n",
    "Example: Predicting a person's height (y) based on their age (x).\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression models the relationship between one dependent variable and two or more independent variables. The equation is an extension of the simple linear model:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "⋯\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +⋯+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "Example: Predicting a person's weight (y) based on their height (x1), age (x2), and daily calorie intake (x3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde18276-e360-4119-935c-a299295d9943",
   "metadata": {},
   "source": [
    "2.Linearity: The relationship between the dependent and independent variables should be linear.\n",
    "Independence: Observations should be independent of each other.\n",
    "Homoscedasticity: The residuals (errors) should have constant variance at every level of the independent variables.\n",
    "Normality: The residuals should be approximately normally distributed.\n",
    "No Multicollinearity: Independent variables should not be highly correlated with each other.\n",
    "Checking Assumptions:\n",
    "\n",
    "Linearity: Use scatterplots to check the relationship between dependent and independent variables.\n",
    "Independence: Check the Durbin-Watson statistic.\n",
    "Homoscedasticity: Plot residuals versus fitted values and look for a random scatter.\n",
    "Normality: Use Q-Q plots of the residuals.\n",
    "Multicollinearity: Check Variance Inflation Factor (VIF) values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55ea8a-50ab-4f9c-88e6-08508cc65445",
   "metadata": {},
   "source": [
    "3.Intercept (\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): The expected value of the dependent variable (y) when all independent variables (x) are zero.\n",
    "Slope (\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): The change in the dependent variable (y) for a one-unit change in the independent variable (x).\n",
    "Example:\n",
    "If we have a model predicting salary (y) based on years of experience (x):\n",
    "\n",
    "Salary\n",
    "=\n",
    "30\n",
    ",\n",
    "000\n",
    "+\n",
    "5\n",
    ",\n",
    "000\n",
    "×\n",
    "Experience\n",
    "Salary=30,000+5,000×Experience\n",
    "\n",
    "Intercept (\n",
    "30\n",
    ",\n",
    "000\n",
    "30,000): The starting salary with 0 years of experience.\n",
    "Slope (\n",
    "5\n",
    ",\n",
    "000\n",
    "5,000): For each additional year of experience, the salary increases by $5,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fa4f0-1bff-4bba-9fde-b9127607c1d2",
   "metadata": {},
   "source": [
    "4.Gradient Descent:\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the parameters of the model to reduce the cost function by moving in the direction of the steepest descent.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Initialize the parameters.\n",
    "Calculate the gradient of the cost function with respect to each parameter.\n",
    "Update the parameters by moving in the direction opposite to the gradient.\n",
    "Repeat until convergence.\n",
    "Application:\n",
    "In linear regression, gradient descent is used to find the optimal values of the coefficients (\n",
    "𝛽\n",
    "β) that minimize the mean squared error between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab4055-9cef-46cc-af53-2ac4b6e10163",
   "metadata": {},
   "source": [
    "5.Multiple Linear Regression:\n",
    "This model predicts a dependent variable based on multiple independent variables. The formula is:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "⋯\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +⋯+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "Difference from Simple Linear Regression:\n",
    "\n",
    "Simple Linear Regression: One predictor variable.\n",
    "Multiple Linear Regression: Two or more predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f6b2c-8532-4fff-8d07-28a584a64d76",
   "metadata": {},
   "source": [
    "6.Multicollinearity:\n",
    "Multicollinearity occurs when independent variables in a regression model are highly correlated. This can cause issues with the estimation of coefficients and their statistical significance.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Variance Inflation Factor (VIF): A VIF value greater than 10 indicates high multicollinearity.\n",
    "Correlation Matrix: Check for high correlation coefficients between predictor variables.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove correlated predictors.\n",
    "Combine correlated predictors into a single predictor (e.g., through principal component analysis).\n",
    "Regularization techniques: Use Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868e57f-785e-4ee6-ab9e-5579d36c63e4",
   "metadata": {},
   "source": [
    "7.Polynomial Regression:\n",
    "Polynomial regression models the relationship between the dependent and independent variables as an nth-degree polynomial. It is used when the relationship is non-linear.\n",
    "\n",
    "Equation:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "⋯\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    " +⋯+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    " +ϵ\n",
    "\n",
    "Difference from Linear Regression:\n",
    "\n",
    "Linear Regression: Assumes a linear relationship.\n",
    "Polynomial Regression: Can model a non-linear relationship by introducing polynomial terms of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924a7ad-507c-4c55-8512-4a96f249ca41",
   "metadata": {},
   "source": [
    "8.Advantages:\n",
    "\n",
    "Flexibility: Can model non-linear relationships more accurately.\n",
    "Better fit: Can provide a better fit to complex data.\n",
    "Disadvantages:\n",
    "\n",
    "Overfitting: High-degree polynomials can lead to overfitting.\n",
    "Complexity: More complex models are harder to interpret and require more computational resources.\n",
    "When to Use Polynomial Regression:\n",
    "\n",
    "When data exhibits a clear non-linear relationship.\n",
    "When the flexibility of the model outweighs the risk of overfitting.\n",
    "Example: Predicting the growth of a plant where the growth rate accelerates at certain stages might be better modeled with polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b1637-d98a-4370-ada7-5790a8433c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
